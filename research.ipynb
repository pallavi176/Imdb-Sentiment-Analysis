{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pallavi.Saxena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Pallavi.Saxena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Pallavi.Saxena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path=\"data\\IMDBDataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(input_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 2), (10000, 2))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=125)\n",
    "train_df.shape,test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    20007\n",
       "negative    19993\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map={'positive': 1, 'negative': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    20007\n",
       "0    19993\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['sentiment'] = train_df['sentiment'].map(label_map)\n",
    "train_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2451</th>\n",
       "      <td>I have to agree with the previous author's com...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29907</th>\n",
       "      <td>Despite an overall pleasing plot and expensive...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14936</th>\n",
       "      <td>'Fame' (1980) is brilliant. It's got all these...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25058</th>\n",
       "      <td>This is a delightful film. Elizabeth Taylor do...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10401</th>\n",
       "      <td>I believe there are two angles to the story, f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "2451   I have to agree with the previous author's com...          1\n",
       "29907  Despite an overall pleasing plot and expensive...          0\n",
       "14936  'Fame' (1980) is brilliant. It's got all these...          1\n",
       "25058  This is a delightful film. Elizabeth Taylor do...          1\n",
       "10401  I believe there are two angles to the story, f...          1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['review'] = train_df['review'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Html Tags\n",
    "def remove_tags(text):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', text)\n",
    "train_df['review'] = train_df['review'].apply(lambda x: remove_tags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove links:\n",
    "def remove_url(txt):\n",
    "    return re.sub(r'\\s*https?://\\S+(\\s+|$)', '', txt, flags=re.MULTILINE)\n",
    "train_df['review'] = train_df['review'].apply(lambda x: remove_url(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Remove hash tag:\n",
    "def remove_hashtag(txt):\n",
    "    return re.sub(r'@[A-Za-z0-9]+', '', txt, flags=re.MULTILINE)\n",
    "train_df['review'] = train_df['review'].apply(lambda x: remove_hashtag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "def remove_punc(txt):\n",
    "    return txt.translate(str.maketrans('', '', string.punctuation))\n",
    "train_df['review'] = train_df['review'].apply(lambda x: remove_punc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Special Char\n",
    "def remove_spchar(text):\n",
    "    return re.sub('\\W+',' ', text)\n",
    "train_df['review'] = train_df['review'].apply(lambda x: remove_spchar(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Non- ascii characters\n",
    "def remove_nonascii(txt):\n",
    "    encoded_string = txt.encode(\"ascii\", \"ignore\")\n",
    "    return encoded_string.decode()\n",
    "train_df['review'] = train_df['review'].apply(lambda x: remove_nonascii(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Stopwords\n",
    "def remove_stopwords(txt):\n",
    "    data = txt.split()\n",
    "    new = [word for word in data if not word in stopwords.words('english')]\n",
    "    return \" \".join(new)\n",
    "train_df['review'] = train_df['review'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying lemmatization\n",
    "def lematize(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return \" \".join([wnl.lemmatize(i,pos='v') for i in text.split()])\n",
    "train_df['review'] = train_df['review'].apply(lambda x: lematize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df[train_df['review'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_preprocess(review):\n",
    "    \"\"\"\n",
    "    Takes in a string of review, then performs the following:\n",
    "    1. Remove HTML tag from review\n",
    "    2. Remove URLs from review\n",
    "    3. Make entire review lowercase\n",
    "    4. Split the review in words\n",
    "    5. Remove all punctuation\n",
    "    6. Remove empty strings from review\n",
    "    7. Remove all stopwords\n",
    "    8. Returns a list of the cleaned review after jioning them back to a sentence\n",
    "    \"\"\"\n",
    "    en_stops = set(stopwords.words('english'))\n",
    "    \n",
    "    \"\"\"\n",
    "    Removing HTML tag from review\n",
    "    \"\"\"\n",
    "    clean = re.compile('<.*?>')\n",
    "    review_without_tag = re.sub(clean, '', review) \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Removing URLs\n",
    "    \"\"\"\n",
    "    review_without_tag_and_url = re.sub(r\"http\\S+\", \"\", review_without_tag)\n",
    "    \n",
    "    review_without_tag_and_url = re.sub(r\"www\\S+\", \"\", review_without_tag)\n",
    "    \n",
    "    \"\"\"\n",
    "    Make entire string lowercase\n",
    "    \"\"\"\n",
    "    review_lowercase = review_without_tag_and_url.lower()\n",
    "    \n",
    "    \"\"\"\n",
    "    Split string into words\n",
    "    \"\"\"\n",
    "    list_of_words = word_tokenize(review_lowercase)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Remove punctuation\n",
    "    Checking characters to see if they are in punctuation\n",
    "    \"\"\"\n",
    "\n",
    "    list_of_words_without_punctuation=[''.join(this_char for this_char in this_string if (this_char in string.ascii_lowercase))for this_string in list_of_words]\n",
    "     \n",
    "    \n",
    "    \"\"\"\n",
    "    Remove empty strings\n",
    "    \"\"\"\n",
    "    list_of_words_without_punctuation = list(filter(None, list_of_words_without_punctuation))\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Remove any stopwords\n",
    "    \"\"\"\n",
    "  \n",
    "    filtered_word_list = [w for w in list_of_words_without_punctuation if w not in en_stops] \n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a list of the cleaned review after jioning them back to a sentence\n",
    "    \"\"\"\n",
    "    return ' '.join(filtered_word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(text):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', text)\n",
    "\n",
    "def remove_url(txt):\n",
    "    return re.sub(r'\\s*https?://\\S+(\\s+|$)', '', txt, flags=re.MULTILINE)\n",
    "\n",
    "def remove_hashtag(txt):\n",
    "    return re.sub(r'@[A-Za-z0-9]+', '', txt, flags=re.MULTILINE)\n",
    "\n",
    "def remove_punc(txt):\n",
    "    return txt.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_spchar(text):\n",
    "    return re.sub('\\W+',' ', text)\n",
    "\n",
    "def remove_nonascii(txt):\n",
    "    encoded_string = txt.encode(\"ascii\", \"ignore\")\n",
    "    return encoded_string.decode()\n",
    "\n",
    "def remove_stopwords(txt):\n",
    "    data = txt.split()\n",
    "    new = [word for word in data if not word in stopwords.words('english')]\n",
    "    return \" \".join(new)\n",
    "\n",
    "def lematize(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return \" \".join([wnl.lemmatize(i,pos='v') for i in text.split()])\n",
    "\n",
    "def preprocess_df(df):\n",
    "    print(df.shape)\n",
    "\n",
    "    # Lower all caps\n",
    "    df['review'] = df['review'].apply(lambda x: x.lower())\n",
    "\n",
    "    # Remove Html Tags\n",
    "    df['review'] = df['review'].apply(lambda x: remove_tags(x))\n",
    "\n",
    "    # Remove links:\n",
    "    df['review'] = df['review'].apply(lambda x: remove_url(x))\n",
    "\n",
    "    # Remove hash tag:\n",
    "    df['review'] = df['review'].apply(lambda x: remove_hashtag(x))\n",
    "\n",
    "    # Remove punctuation\n",
    "    df['review'] = df['review'].apply(lambda x: remove_punc(x))\n",
    "\n",
    "    # Remove Special Char\n",
    "    df['review'] = df['review'].apply(lambda x: remove_spchar(x))\n",
    "\n",
    "    #Remove Non- ascii characters\n",
    "    df['review'] = df['review'].apply(lambda x: remove_nonascii(x))\n",
    "\n",
    "    #Remove Stopwords\n",
    "    #df['review'] = df['review'].apply(lambda x: remove_stopwords(x))\n",
    "    \n",
    "    # Applying lemmatization\n",
    "    df['review'] = df['review'].apply(lambda x: lematize(x))\n",
    "\n",
    "    df = df[df['review'].notna()]\n",
    "    print(df.shape)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df, vocab):\n",
    "    review_dict={'neg':[],'pos':[]}\n",
    "    for label_type in [0, 1]: \n",
    "        clean_df=review_preprocess(df)\n",
    "        if label_type == 'neg':\n",
    "            review_dict['neg'].append(clean_review)\n",
    "        else:\n",
    "            review_dict['pos'].append(clean_review)\n",
    "        # Update counts\n",
    "        vocab.update(clean_review.split())\n",
    "                        \n",
    "    return review_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 2), (10000, 2))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(input_path)\n",
    "label_map={'positive': 1, 'negative': 0}\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=125)\n",
    "train_df.shape,test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    20007\n",
       "0    19993\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['sentiment'] = train_df['sentiment'].map(label_map)\n",
    "train_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5007\n",
       "1    4993\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['sentiment'] = test_df['sentiment'].map(label_map)\n",
    "test_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I have to agree with the previous author's comments about the excellent performances and plot. Started watching this movie by accident...(lazy Sunday afternoon clicking channels to see if anything good was on)...and was mesmerized by Martin Sheen and Emilio Estevez. Wow! Gut wrenching! Kudos to everyone (have always admired Martin Sheen) but was particularly impressed with Emilio! Excellent job of acting and directing...simply superb! So why have I never heard of this movie before? I'll have to spread the news.\",\n",
       " 'Despite an overall pleasing plot and expensive production one wonders how a director can make so many clumsy cultural mistakes. Where were the Japanese wardrobe and cultural consultants? Not on the payroll apparently. <br /><br />A Japanese friend of mine actually laughed out loud at some of the cultural absurdities she watched unfold before her eyes. In a later conversation she said, \"Imagine a Finnish director making a movie in Fnnish about the American Civil War using blond Swedish actors as union Army and Frenchmen as the Confederates. Worse imagine dressing the Scarlet O\\'Hara female lead in a period hoop skirt missing the hoop and sporting a 1950\\'s hairdo. Maybe some people in Finland might not realize that the hoop skirt was \"missing the hoop\" or recognize the bizarre Jane Mansfield hair, but in Atlanta they would not believe their eyes or ears....and be laughing in the aisles...excellent story and photography be damned.<br /><br />So...watching Memoirs of a Geisha was painful for anyone familiar with Japanese cultural nuances, actual geisha or Japanese dress, and that was the topic of the movie! Hollywood is amazing in its myopic view of film making. They frequently get the big money things right while letting the details that really polish a films refinement embarrassingly wrong. I thought \"The Last Samurai\" was the crowning achievement of how bad an otherwise good film on Japan could be. Memoirs of a Geisha is embarrassingly better and worse at the same time.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_review = train_df['review'].tolist()\n",
    "test_review = test_df['review'].tolist()\n",
    "train_review[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m vocab \u001b[39m=\u001b[39m Counter()\n\u001b[1;32m----> 2\u001b[0m train_review_dict\u001b[39m=\u001b[39mget_data(train_review, vocab)\n\u001b[0;32m      3\u001b[0m test_review_dict\u001b[39m=\u001b[39mget_data(test_review, vocab)\n",
      "Cell \u001b[1;32mIn [44], line 4\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(review, vocab)\u001b[0m\n\u001b[0;32m      2\u001b[0m review_dict\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mneg\u001b[39m\u001b[39m'\u001b[39m:[],\u001b[39m'\u001b[39m\u001b[39mpos\u001b[39m\u001b[39m'\u001b[39m:[]}\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m label_type \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mneg\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpos\u001b[39m\u001b[39m'\u001b[39m]: \n\u001b[1;32m----> 4\u001b[0m     clean_review\u001b[39m=\u001b[39mreview_preprocess(review)\n\u001b[0;32m      5\u001b[0m     \u001b[39mif\u001b[39;00m label_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mneg\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m      6\u001b[0m         review_dict[\u001b[39m'\u001b[39m\u001b[39mneg\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(clean_review)\n",
      "Cell \u001b[1;32mIn [38], line 19\u001b[0m, in \u001b[0;36mreview_preprocess\u001b[1;34m(review)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mRemoving HTML tag from review\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m clean \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(\u001b[39m'\u001b[39m\u001b[39m<.*?>\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m review_without_tag \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39;49msub(clean, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, review) \n\u001b[0;32m     22\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39mRemoving URLs\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m review_without_tag_and_url \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttp\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mS+\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, review_without_tag)\n",
      "File \u001b[1;32mc:\\AI\\NLP\\NLPProjects\\NLive\\Imdb_SentimentAnalysis\\CodeBase\\Imdb-Sentiment-Analysis\\env\\lib\\re.py:210\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msub\u001b[39m(pattern, repl, string, count\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m    204\u001b[0m     \u001b[39m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[39m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[39m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39;49msub(repl, string, count)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "vocab = Counter()\n",
    "train_review_dict=get_data(train_review, vocab)\n",
    "test_review_dict=get_data(test_review, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e42e151192dc493ee9250360506afe91b6a050f2ab580d25fa8fef40c60d9c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
